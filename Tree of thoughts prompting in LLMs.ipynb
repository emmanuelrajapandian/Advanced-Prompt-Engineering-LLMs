{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW1moHJ1TdhO"
      },
      "source": [
        "# Tree-of-thoughts prompting in LLMs\n",
        "\n",
        "In this notebook, I'll employ the [Mixtral8x7B-Instruct](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1) LLM model to explore and apply tree-of-thought prompting technique. This involves practically learning these methods by applying on various examples.\n",
        "\n",
        "This  LLM is capable enough to run on google colab thanks to **Denis Mazur** and **Artyom Eliseev** who have made this possible by quantizing the original model in mixed precision and implementing a MoE-specific offloading strategy. `[Quantization, build_model, Expert MoE files were forked from their github repo]`.\n",
        "\n",
        "Read their amazing [tech report](https://arxiv.org/abs/2312.17238). I will try to summarize my learnings from their report in the following days in this notebook.\n",
        "\n",
        "\n",
        "##### *Edited: 01-09-2024.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8MhvkC7TKEL"
      },
      "source": [
        "## Install and import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Cm6njCbRHGja",
        "outputId": "2c798a03-6d89-44fb-de5d-11a6762cd0a0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<script>\n",
              "code_show_err=false;\n",
              "function code_toggle_err() {\n",
              " if (code_show_err){\n",
              " $('div.output_stderr').hide();\n",
              " } else {\n",
              " $('div.output_stderr').show();\n",
              " }\n",
              " code_show_err = !code_show_err\n",
              "}\n",
              "$( document ).ready(code_toggle_err);\n",
              "</script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import HTML, display, Markdown\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)\n",
        "\n",
        "HTML('''<script>\n",
        "code_show_err=false;\n",
        "function code_toggle_err() {\n",
        " if (code_show_err){\n",
        " $('div.output_stderr').hide();\n",
        " } else {\n",
        " $('div.output_stderr').show();\n",
        " }\n",
        " code_show_err = !code_show_err\n",
        "}\n",
        "$( document ).ready(code_toggle_err);\n",
        "</script>''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "f7qY7ebqX7T7"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# fix triton in colab\n",
        "!export LC_ALL=\"en_US.UTF-8\"\n",
        "!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
        "!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
        "!ldconfig /usr/lib64-nvidia\n",
        "\n",
        "!git clone https://github.com/emmanuelrajapandian/Advanced-Prompt-Engineering-LLMs.git --quiet\n",
        "!cd Advanced-Prompt-Engineering-LLMs && pip install -q -r requirements.txt\n",
        "!huggingface-cli download lavawolfiee/Mixtral-8x7B-Instruct-v0.1-offloading-demo --quiet --local-dir Mixtral-8x7B-Instruct-v0.1-offloading-demo\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "c3b3bbe7e5b7489e932df2f63d8bb651",
            "739dc680aa6945e7aee71782cff7b3ea",
            "baf30f1670534c999576bce78d9e635c",
            "346a7af2bc9c441e9f9a86ede2ae7922",
            "5c5909da8d8841fa96b81cd5991bb4ce",
            "500bfdbf958e47d9895a8dfc8e31ffac",
            "b82e453f198d4bd18169e58903c06014",
            "822978bccaee433689e7376f9c96cae8",
            "8ec5ed0db3784ea583d045661805f39a",
            "e086c129ad7b47acb79c4075557cf65c",
            "6cca8423b6974309a7acd8ae07e129c7"
          ]
        },
        "id": "GgpjnV7fV49W",
        "outputId": "2abef442-f711-484f-c31c-e9b7288dd1dc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hqq_aten package not installed. HQQBackend.ATEN backend will not work unless you install the hqq_aten lib in hqq/kernels.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3b3bbe7e5b7489e932df2f63d8bb651",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append(\"Advanced-Prompt-Engineering-LLMs\")\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from hqq.core.quantize import BaseQuantizeConfig\n",
        "from langchain.prompts import PromptTemplate\n",
        "from huggingface_hub import snapshot_download\n",
        "from IPython.display import clear_output\n",
        "from tqdm.auto import trange\n",
        "from transformers import AutoConfig, AutoTokenizer\n",
        "from transformers.utils import logging as hf_logging\n",
        "\n",
        "from source.build_model import OffloadConfig, QuantConfig, build_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkSYibHcTQsH"
      },
      "source": [
        "## Initialize model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_mIpePTMFyRY"
      },
      "outputs": [],
      "source": [
        "model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "quantized_model_name = \"lavawolfiee/Mixtral-8x7B-Instruct-v0.1-offloading-demo\"\n",
        "state_path = \"Mixtral-8x7B-Instruct-v0.1-offloading-demo\"\n",
        "\n",
        "config = AutoConfig.from_pretrained(quantized_model_name)\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "offload_per_layer = 4\n",
        "num_experts = config.num_local_experts\n",
        "\n",
        "offload_config = OffloadConfig(\n",
        "    main_size=config.num_hidden_layers * (num_experts - offload_per_layer),\n",
        "    offload_size=config.num_hidden_layers * offload_per_layer,\n",
        "    buffer_size=4,\n",
        "    offload_per_layer=offload_per_layer,\n",
        ")\n",
        "\n",
        "\n",
        "attn_config = BaseQuantizeConfig(\n",
        "    nbits=4,\n",
        "    group_size=64,\n",
        "    quant_zero=True,\n",
        "    quant_scale=True,\n",
        ")\n",
        "\n",
        "attn_config[\"scale_quant_params\"][\"group_size\"] = 256\n",
        "\n",
        "ffn_config = BaseQuantizeConfig(\n",
        "    nbits=2,\n",
        "    group_size=16,\n",
        "    quant_zero=True,\n",
        "    quant_scale=True,\n",
        ")\n",
        "quant_config = QuantConfig(ffn_config=ffn_config, attn_config=attn_config)\n",
        "\n",
        "\n",
        "# Building the Model after specifying the params in the instantiated Classes\n",
        "# OffloadConfig(), BaseQuantizeConfig().\n",
        "model = build_model(\n",
        "    device=device,\n",
        "    quant_config=quant_config,\n",
        "    offload_config=offload_config,\n",
        "    state_path=state_path,\n",
        ")\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4hBFYtPTUzT"
      },
      "source": [
        "## Run the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Zf4GkspecSm8"
      },
      "outputs": [],
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "past_key_values = None\n",
        "sequence = None\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "HIKlmO4ulryT",
        "outputId": "250a1026-6f91-4c41-d8d8-a33cbbfe5f4f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def generate_outputs(prompt:str, temperature:float, max_tokens:int=1000, do_sample:bool=True, stop_sequences:list=[], n:int=1) -> list:\n",
        "    \"\"\"\n",
        "    Function to run inference with Mixtral model\n",
        "    \"\"\"\n",
        "    user_entry = dict(role=\"user\", content=prompt)\n",
        "    input_ids = tokenizer.apply_chat_template([user_entry], return_tensors=\"pt\").to(device)\n",
        "\n",
        "    outputs = []\n",
        "    for i in range(n):\n",
        "        outputs.append(model.generate(input_ids=input_ids,\n",
        "                                      streamer=streamer,\n",
        "                                      do_sample=do_sample,\n",
        "                                      temperature=temperature,\n",
        "                                      top_p=0.9,\n",
        "                                      max_new_tokens=max_tokens,\n",
        "                                      pad_token_id=tokenizer.eos_token_id,\n",
        "                                      return_dict_in_generate=True,\n",
        "                                      output_hidden_states=False,))\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiSqTQIIfFmU"
      },
      "outputs": [],
      "source": [
        "!pip install igraph --q\n",
        "!pip install plotly --q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "MZ8sPI55dBlH",
        "outputId": "0c2680ef-b937-46bd-9062-c3eea2c50c78"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from totclasscode import TextTaskToT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "0QC3Zwl1dBh2",
        "outputId": "1e42e5d0-ff65-4fb5-e220-226f06d1e680"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "title = \"Impact of Artificial Intelligence (AI) on Job Market\"\n",
        "start = \"AI will affect almost 40 percent of jobs around the world, replacing some and complementing others.\"\n",
        "mid = '''\" more opportunities to leverage AI benefits\"'''\n",
        "end = '''\"The AI era is upon us, and it is still within our power to ensure it brings prosperity for all.\"'''\n",
        "\n",
        "input_data = (title, start, mid, end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "dj-XiqPIdBfV",
        "outputId": "4c06fb12-8f45-429e-eec6-59a4ce1887df"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "TEMP = 0.75\n",
        "output = TextTaskToT(input_data, temperature= TEMP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "LwaALNAti26m",
        "outputId": "5eef0d87-2027-4b19-9148-bc1dcafc73db"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Human: Write a passage of 4 paragraphs.\n",
            "\n",
            "The first line must contain only this title:\n",
            "Impact of Artificial Intelligence (AI) on Job Market\n",
            "\n",
            "The first paragraph must start with sentence:\n",
            "AI will affect almost 40 percent of jobs around the world, replacing some and complementing others.\n",
            "\n",
            "The second paragram must contain the words:\n",
            "\" more opportunities to leverage AI benefits\"\n",
            "\n",
            "The last paragraph must end with sentence:\n",
            "\"The AI era is upon us, and it is still within our power to ensure it brings prosperity for all.\"\n",
            "\n",
            "Assistant:\n"
          ]
        }
      ],
      "source": [
        "prompt = output.wrap_prompt(\"standard\")\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbiboNBFwLRc"
      },
      "source": [
        "Standard prompting of an Article regarding AI and Jobs using a template predefined in python file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "VRy34JGFdBc0",
        "outputId": "a1e49b9a-4ee6-49fc-c514-e1f2f43a23ed"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Impact of Artificial Intelligence (AI) on Job Market\n",
            "\n",
            "AI will affect almost 40 percent of jobs around the world, replacing some and complementing others. The advent of AI in the job market is a significant development, one that is bound to bring about substantial changes in the way we perceive and approach work. It's estimated that around 3,500 major corporate jobs could be automated in the United Kingdom alone, according to a recent study. This statistic underscores the immediate and potential impact of AI on employment.\n",
            "\n",
            "However, it's important to note that AI won't solely be a force of reduction in the job market. Instead, it will provide more opportunities to leverage AI benefits. The integration of AI in various industries will lead to the creation of new jobs that require a unique blend of human and machine capabilities. These new roles will be geared towards managing, maintaining, and operating AI technologies, necessitating a significant shift in the skill sets and competencies demanded by employers. Thus, while AI may replace certain jobs, it will also create new ones, shifting the focus to more specialized and AI-centric roles.\n",
            "\n",
            "The middle ground, where AI complements and augments human labor, is perhaps the most promising. For instance, AI can take over mundane, repetitive tasks, freeing up time for employees to focus on higher-level tasks that require creativity, critical thinking, and emotional intelligence. This symbiotic relationship allows for a more efficient allocation of resources and a better distribution of workload. Moreover, AI can also provide valuable insights and predictions, enabling businesses to make more informed decisions and enhancing overall productivity.\n",
            "\n",
            "In conclusion, the AI era is upon us, and it is still within our power to ensure it brings prosperity for all. To achieve this, we must embrace the potential of AI while addressing the inherent challenges, such as skill gaps and job displacement. This will necessitate a concerted effort from governments, businesses, and educators to provide adequate training, education, and support for the workforce. By doing so, we can create a more inclusive and equitable future, where AI serves as a tool to empower and uplift, rather than a mere replacement for human labor.\n"
          ]
        }
      ],
      "source": [
        "result = output.make_passages(method= \"standard\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "346a7af2bc9c441e9f9a86ede2ae7922": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e086c129ad7b47acb79c4075557cf65c",
            "placeholder": "​",
            "style": "IPY_MODEL_6cca8423b6974309a7acd8ae07e129c7",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "500bfdbf958e47d9895a8dfc8e31ffac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c5909da8d8841fa96b81cd5991bb4ce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cca8423b6974309a7acd8ae07e129c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "739dc680aa6945e7aee71782cff7b3ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_500bfdbf958e47d9895a8dfc8e31ffac",
            "placeholder": "​",
            "style": "IPY_MODEL_b82e453f198d4bd18169e58903c06014",
            "value": ""
          }
        },
        "822978bccaee433689e7376f9c96cae8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "8ec5ed0db3784ea583d045661805f39a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b82e453f198d4bd18169e58903c06014": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "baf30f1670534c999576bce78d9e635c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_822978bccaee433689e7376f9c96cae8",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ec5ed0db3784ea583d045661805f39a",
            "value": 0
          }
        },
        "c3b3bbe7e5b7489e932df2f63d8bb651": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_739dc680aa6945e7aee71782cff7b3ea",
              "IPY_MODEL_baf30f1670534c999576bce78d9e635c",
              "IPY_MODEL_346a7af2bc9c441e9f9a86ede2ae7922"
            ],
            "layout": "IPY_MODEL_5c5909da8d8841fa96b81cd5991bb4ce"
          }
        },
        "e086c129ad7b47acb79c4075557cf65c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
